{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3onTvrp7CjmF"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "wq3bFntRCjmJ",
        "outputId": "dcc8fc09-3351-4dd3-fdd6-abe879b535dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: could not create work tree dir 'REPO_NAME': No such file or directory\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/REPO_NAME'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2803080359.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Move into repo directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/REPO_NAME\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mNOTEBOOK_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/REPO_NAME'"
          ]
        }
      ],
      "source": [
        "#Import packages\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot  as plt\n",
        "import shapely.geometry as shp\n",
        "from shapely.ops import linemerge\n",
        "import numpy as np\n",
        "import itertools\n",
        "import pickle\n",
        "import matplotlib as mpl\n",
        "import matplotlib.lines as mlines\n",
        "\n",
        "# Define notebook path with pathlib\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "\n",
        "# Clone repo if not already present\n",
        "if not os.path.exists(\"/content/REPO_NAME\"):\n",
        "    !git clone https://github.com/YOUR_USERNAME/REPO_NAME.git\n",
        "\n",
        "# Move into repo directory\n",
        "os.chdir(\"/content/REPO_NAME\")\n",
        "\n",
        "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
        "DATA_PATH = NOTEBOOK_PATH / \"data\"\n",
        "\n",
        "\n",
        "NOTEBOOK_PATH = pathlib.Path().resolve()\n",
        "DATA_PATH = NOTEBOOK_PATH / \"data\"\n",
        "\n",
        "\n",
        "# Import France Map\n",
        "# France = pd.read_pickle(DATA_PATH/'France_metropolitaine.pkl')\n",
        "# Attempt to load with latin1 encoding to handle potential Python 2/3 pickle compatibility issues\n",
        "with open(DATA_PATH/'France_metropolitaine.pkl', 'rb') as file:\n",
        "    France = pickle.load(file, encoding='latin1')\n",
        "\n",
        "# Import capture and storage sites\n",
        "capture_sites           = pd.read_pickle(DATA_PATH/'data_clean'/'capture_sites.pkl')\n",
        "storage_sites           = pd.read_pickle(DATA_PATH/'data_clean'/'storage_sites.pkl')\n",
        "edges_gdf               = pd.read_pickle(DATA_PATH/'data_clean'/'edges_gdf.pkl')\n",
        "incidence               = pd.read_pickle(DATA_PATH/'data_clean'/'incidence.pkl')\n",
        "nodes                   = pd.read_pickle(DATA_PATH/'data_clean'/'nodes.pkl')\n",
        "\n",
        "# Import scenario\n",
        "with open(NOTEBOOK_PATH/'results'/'MILP_results.pkl','rb') as file:\n",
        "    res = pickle.load(file)\n",
        "\n",
        "# Multiple legends for figures\n",
        "from matplotlib.colors import Normalize\n",
        "from matplotlib.cm import ScalarMappable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvsfIzhSCjmK"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZFvOnroCjmL"
      },
      "source": [
        "## Qpos, Qneg and delta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81WwwjorCjmL"
      },
      "outputs": [],
      "source": [
        "def Var_pipe(variable):\n",
        "    name = str(variable)\n",
        "    var = res[ \"v_\" + name ]\n",
        "    var.rename(columns=\n",
        "                {'index':'pipeline'\n",
        "                ,0: name}\n",
        "                , inplace= True\n",
        "    )\n",
        "    var = var.drop(var.loc[ var[name] ==0 ].index)\n",
        "    return var\n",
        "\n",
        "qpos = Var_pipe(\"qpositive\")\n",
        "qneg = Var_pipe(\"qnegative\")\n",
        "delta = Var_pipe(\"delta\")\n",
        "\n",
        "\n",
        "# Verification\n",
        "len(delta) == len(qpos) + len(qneg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diz_XS15CjmM"
      },
      "source": [
        "## Total costs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etPB2sS6CjmM"
      },
      "outputs": [],
      "source": [
        "# Total cost\n",
        "capex = res[\"v_TotalPipeCapex\"]\n",
        "opex  = res[\"v_TotalPipeOpex\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHh3rBiZCjmN"
      },
      "source": [
        "## Qtrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTHfdW8ECjmN"
      },
      "outputs": [],
      "source": [
        "# Merge qpos and qneg quantities with geodata\n",
        "qpos = qpos.merge(edges_gdf.reset_index(names=\"pipeline\"),\n",
        "           how=\"inner\",\n",
        "           on=\"pipeline\",\n",
        "           validate = \"1:1\"\n",
        ")\n",
        "qneg = qneg.merge(edges_gdf.reset_index(names=\"pipeline\"),\n",
        "           how=\"inner\",\n",
        "           on=\"pipeline\",\n",
        "           validate = \"1:1\"\n",
        ")\n",
        "\n",
        "# Convert both dfs into geo dfs\n",
        "qpos = gpd.GeoDataFrame(\n",
        "        qpos,\n",
        "        crs=\"EPSG:4326\",\n",
        "        geometry = \"geometry\"\n",
        ")\n",
        "qneg = gpd.GeoDataFrame(\n",
        "        qneg,\n",
        "        crs=\"EPSG:4326\",\n",
        "        geometry = \"geometry\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18QFGctJCjmN"
      },
      "outputs": [],
      "source": [
        "# Concat the gdfs into a single gdf\n",
        "qtrans_pos = qpos.rename(columns={\"qpositive\":\"qtrans\"})\n",
        "qtrans_neg = qneg.rename(columns={\"qnegative\":\"qtrans\"})\n",
        "\n",
        "qtrans_pos[\"type\"] = qtrans_pos[\"qtrans\"] / qtrans_pos[\"qtrans\"]\n",
        "qtrans_neg[\"type\"] = - qtrans_neg[\"qtrans\"] / qtrans_neg[\"qtrans\"]\n",
        "\n",
        "\n",
        "qtrans = gpd.GeoDataFrame(\n",
        "    pd.concat([qtrans_pos,qtrans_neg],axis=0,join='outer',ignore_index=True),\n",
        "    crs = qpos.crs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V373Rhs1CjmO"
      },
      "outputs": [],
      "source": [
        "d = 0.07\n",
        "sum = 0\n",
        "\n",
        "Cfix_morbee = 0.533             # in [M€/km]\n",
        "Cvol_morbee = 0.019             # in [M€ / (km * (MtCO2/y)]\n",
        "\n",
        "for i in range(0,30):\n",
        "    sum = sum + 1/(1+d)**i\n",
        "sum\n",
        "\n",
        "Cfix = Cfix_morbee / sum        # Annualized invetment fix cost [M€/km/y]\n",
        "Cvol = Cvol_morbee / sum        # Annualized investment variable cost   [M€/km/(MtCO2/y) / y]\n",
        "Copex = 0.01                    # O&m cost [M€/MtCO2/km]\n",
        "\n",
        "print(Cfix)\n",
        "print(Cvol)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmGJxk72CjmO"
      },
      "outputs": [],
      "source": [
        "# Reminder: qtrans is in [MtCO2/y]\n",
        "\n",
        "# Change crs to projected crs to calculate distances\n",
        "pipe_cost = qtrans.to_crs(\"EPSG:2154\")\n",
        "pipe_cost[\"length(km)\"] = pipe_cost[\"geometry\"].length / 1e3 # distance in [km]\n",
        "pipe_cost[\"pipe_capex\"] = (Cfix + Cvol * pipe_cost[\"qtrans\"] ) *pipe_cost[\"length(km)\"] # CAPEX in [M€/y]\n",
        "pipe_cost[\"pipe_opex\"] = Copex * pipe_cost[\"qtrans\"] * pipe_cost[\"length(km)\"] # OPEX in [M€/y]\n",
        "pipe_cost[\"pipe_tc\"] = pipe_cost[\"pipe_capex\"] + pipe_cost[\"pipe_opex\"]\n",
        "\n",
        "# Verification\n",
        "# summing over all pipelines we find capex and opex of total cost so OK\n",
        "\n",
        "# Switch back to geographical CRS\n",
        "pipe_cost = pipe_cost.to_crs(\"EPSG:4326\")\n",
        "\n",
        "# Verify if there are duplicated pipelines\n",
        "# if there are, it is a mistake\n",
        "# pipe_cost[pipe_cost[\"pipeline\"].duplicated(keep=False)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y5iNHs7CjmO"
      },
      "outputs": [],
      "source": [
        "start_pts = incidence.loc[incidence[\"inc\"]==-1]\n",
        "end_pts   = incidence.loc[incidence[\"inc\"]==1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5komJOgiCjmP"
      },
      "outputs": [],
      "source": [
        "\n",
        "pipe_cost = pipe_cost.merge(\n",
        "    start_pts[[\"pipelines\",\"nodes\",\"nodes_geom\"]],\n",
        "    left_on=\"pipeline\",\n",
        "    right_on=\"pipelines\",\n",
        "    how=\"inner\",\n",
        "    validate= \"1:1\"\n",
        ")\n",
        "pipe_cost.drop(columns=\"pipelines\",inplace=True)\n",
        "pipe_cost.rename(columns=\n",
        "        {\"nodes\":\"start_pt\",\n",
        "         \"nodes_geom\":\"start_pt_geom\"},\n",
        "         inplace=True\n",
        ")\n",
        "\n",
        "pipe_cost = pipe_cost.merge(\n",
        "    end_pts[[\"pipelines\",\"nodes\",\"nodes_geom\"]],\n",
        "    left_on=\"pipeline\",\n",
        "    right_on=\"pipelines\",\n",
        "    how=\"inner\",\n",
        "    validate= \"1:1\"\n",
        ")\n",
        "pipe_cost.drop(columns=\"pipelines\",inplace=True)\n",
        "pipe_cost.rename(columns=\n",
        "        {\"nodes\":\"end_pt\",\n",
        "         \"nodes_geom\":\"end_pt_geom\"},\n",
        "         inplace=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5eWqbm0CjmP"
      },
      "outputs": [],
      "source": [
        "# Set starting points as effective starting points of pipelines\n",
        "# We change the value when the flow is qneg\n",
        "qneg_index = (pipe_cost[\"type\"]<0)\n",
        "pipe_cost.loc[qneg_index,[\"start_pt\",\"end_pt\"]] = pipe_cost.loc[qneg_index,[\"end_pt\",\"start_pt\"]].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBvI8Xk0CjmP"
      },
      "outputs": [],
      "source": [
        "# Add capture cost\n",
        "pipe_cost = pipe_cost.merge(\n",
        "    capture_sites[[\"captured_emi(MtCO2/y)\",\"cost(tCO2_avoided)\"]],\n",
        "    left_on=\"start_pt\",\n",
        "    right_index=True,\n",
        "    how=\"inner\",\n",
        "    # validate=\"1:1\"\n",
        ")\n",
        "pipe_cost[\"capture_tc\"]  = pipe_cost[\"captured_emi(MtCO2/y)\"] * pipe_cost[\"cost(tCO2_avoided)\"]\n",
        "\n",
        "# capture_tc is in [M€/y]\n",
        "# [MtCO2/y] * [€/tCO2avoided]\n",
        "\n",
        "# Re order columns\n",
        "cols = [\"pipeline\",\"start_pt\",\"end_pt\",\"qtrans\",\"capture_tc\",\n",
        "        \"pipe_tc\",\"pipe_capex\",\"pipe_opex\",\"start_pt_geom\",\"end_pt_geom\",\n",
        "        \"geometry\",\"length(km)\",\"cost(tCO2_avoided)\",\"captured_emi(MtCO2/y)\",\"type\"]\n",
        "pipe_cost = pipe_cost[cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_zvLI5ACjmP"
      },
      "source": [
        "# Clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9tMdrPiCjmP"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create a dictionary to map each point to its connected storage point\n",
        "connected_storage = {}\n",
        "\n",
        "def dfs(point, storage_point, visited):\n",
        "    visited.add(point)\n",
        "    connected_storage[point] = storage_point\n",
        "\n",
        "    for _, row in pipe_cost.iterrows():\n",
        "        if row['start_pt'] == point and row['end_pt'] not in visited:\n",
        "            dfs(row['end_pt'], storage_point, visited)\n",
        "        elif row['end_pt'] == point and row['start_pt'] not in visited:\n",
        "            dfs(row['start_pt'], storage_point, visited)\n",
        "\n",
        "# Step 2: Traverse the graph using DFS and assign clusters based on connected storage points\n",
        "visited_points = set()\n",
        "\n",
        "for _, row in pipe_cost.iterrows():\n",
        "    if row['end_pt'].startswith('S') and row['end_pt'] not in visited_points:\n",
        "        dfs(row['end_pt'], row['end_pt'], visited_points)\n",
        "\n",
        "# Step 3: Create a dictionary to map each pipeline to its cluster\n",
        "pipeline_to_cluster = {pipeline: connected_storage[end_pt] for pipeline, end_pt in zip(pipe_cost['pipeline'], pipe_cost['end_pt'])}\n",
        "\n",
        "# Step 4: Assign clusters to GeoDataFrame\n",
        "pipe_cost['cluster'] = pipe_cost['pipeline'].apply(lambda x: pipeline_to_cluster.get(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lkj3hiXkCjmP"
      },
      "outputs": [],
      "source": [
        "capture_sites = capture_sites.reset_index(names=\"capture_name\").set_index(\"capture_name\")\n",
        "capture_sites = capture_sites.drop(columns=\"index\")\n",
        "\n",
        "capture_sites = capture_sites.merge(\n",
        "    pipe_cost[[\"start_pt\",\"cluster\"]],\n",
        "    how=\"inner\",\n",
        "    left_on=\"capture_name\",\n",
        "    right_on=\"start_pt\",\n",
        "    validate=\"1:1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOkz0Vn5CjmP"
      },
      "source": [
        "## Clusters- S0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXcVsfkgCjmP"
      },
      "outputs": [],
      "source": [
        "# We want to plot capture sites that are in specific French regions and in the S0 cluster\n",
        "# We select capture sites in regions under concern\n",
        "capture_S0_regions = capture_sites.loc[(capture_sites[\"geometry\"].within(France.loc[\"Bourgogne-Franche-Comté\",\"geometry\"]))\n",
        "                  | (capture_sites[\"geometry\"].within(France.loc[\"Auvergne-Rhône-Alpes\",\"geometry\"]))]\n",
        "\n",
        "# Among these sites, we only keep those that are in S0 cluster\n",
        "# (in this case they are all in S0)\n",
        "capture_S0_regions = capture_S0_regions.loc[capture_S0_regions[\"cluster\"]==\"S0\"]\n",
        "\n",
        "\n",
        "# We repeat with pipelines that are in the regions and S0 cluster\n",
        "pipelines_S0_regions = pipe_cost.loc[pipe_cost.within(France.loc[\"Bourgogne-Franche-Comté\",\"geometry\"].\n",
        "                                          union(France.loc[\"Auvergne-Rhône-Alpes\",\"geometry\"]))]\n",
        "\n",
        "# Among these pipelines, we only keep those that are in S0 cluster\n",
        "# (in this case they are all in S0)\n",
        "pipelines_S0_regions = pipelines_S0_regions.loc[pipelines_S0_regions[\"cluster\"]==\"S0\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04B1UuKkCjmQ"
      },
      "outputs": [],
      "source": [
        "pipelines_S0_regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4yGlGr8CjmQ"
      },
      "source": [
        "# Figure 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko9rAwb-CjmQ"
      },
      "source": [
        "## Figure 4 - Main map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxhZpo0ICjmQ"
      },
      "outputs": [],
      "source": [
        "# Create custom legend for plots\n",
        "def custom_legend_handler(capture_sites, cmap):\n",
        "    handles = []\n",
        "    unique_categories = capture_sites['original_inventory_sector'].unique()\n",
        "    norm = Normalize(vmin=0, vmax=len(unique_categories) - 1)\n",
        "    scalar_map = ScalarMappable(norm=norm, cmap=cmap)\n",
        "    for idx, category in enumerate(sorted(unique_categories)):  # Sort categories based on colormap order\n",
        "        handle = plt.scatter([], [], label=category, color=scalar_map.to_rgba(idx))\n",
        "        handles.append(handle)\n",
        "    return handles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVDE_1iBCjmQ"
      },
      "outputs": [],
      "source": [
        "# Map emitters all\n",
        "fig, axes = plt.subplots()\n",
        "plt.axis('off')\n",
        "\n",
        "## France\n",
        "France.plot(color='silver', ax=axes)\n",
        "\n",
        "# Capture sites\n",
        "capture_sites.plot(\n",
        "    ax=axes,\n",
        "    column=\"original_inventory_sector\",\n",
        "    cmap=\"seismic\",\n",
        "    markersize=capture_sites['emissions_quantity'] / 1e4,\n",
        "    legend=False,\n",
        ")\n",
        "\n",
        "# Create custom legend for capture sites\n",
        "capture_legend_handles = custom_legend_handler(capture_sites, \"seismic\")\n",
        "\n",
        "\n",
        "# Plot Pipelines\n",
        "qtrans.plot(\n",
        "    ax=axes,\n",
        "    color='black',\n",
        "    linewidth=qtrans[\"qtrans\"],\n",
        "    zorder=1\n",
        ")\n",
        "\n",
        "# Storage sites\n",
        "storage_sites.plot(\n",
        "    ax=axes,\n",
        "    color='green',\n",
        "    marker=\"D\",\n",
        "    markersize=60,\n",
        "    zorder=2\n",
        ")\n",
        "\n",
        "\n",
        "# Legend handles\n",
        "# pipeline_handle = mlines.Line2D([], [], color='black', label='Pipeline route')\n",
        "storage_handle = plt.scatter([], [], color='green', label='storage site', marker='D', s=50)\n",
        "# legend_handles = capture_legend_handles + [storage_handle, pipeline_handle]\n",
        "legend_handles = capture_legend_handles + [storage_handle]\n",
        "\n",
        "# Legend Box\n",
        "plt.legend(\n",
        "    handles=legend_handles,\n",
        "    loc='lower left',\n",
        "    bbox_to_anchor=(-0.45, 0.2),\n",
        "    title=\"Legend\"\n",
        ")\n",
        "\n",
        "# plt.show()\n",
        "# plt.savefig('Map_emitters.png',bbox_inches = \"tight\")\n",
        "plt.savefig(NOTEBOOK_PATH/'results'/'figures'/'Figure_4_main.png', bbox_inches = \"tight\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6nWzD5lCjmQ"
      },
      "source": [
        "## Figure 4 - Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhE6FZiyCjmR"
      },
      "outputs": [],
      "source": [
        "n_lines = 6 # there are six orignal inventory sectors\n",
        "cmap = mpl.colormaps['seismic']\n",
        "\n",
        "# Take colors at regular intervals spanning the colormap.\n",
        "colors = cmap(np.linspace(0, 1, n_lines))\n",
        "\n",
        "# Define all unique sectors in the right order\n",
        "unique_sectors = ['cement', 'chemicals', 'oil-and-gas-refining', 'petrochemicals', 'pulp-and-paper', 'steel']\n",
        "\n",
        "# Create a dictionary associating sectors with colors\n",
        "sector_color_dict = dict(zip(unique_sectors, colors))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GgFUCg-CjmR"
      },
      "outputs": [],
      "source": [
        "\n",
        "fig, axes = plt.subplots()\n",
        "plt.axis('off')\n",
        "\n",
        "# France\n",
        "France.loc[[\"Bourgogne-Franche-Comté\",\"Auvergne-Rhône-Alpes\"],\"geometry\"].plot(color='silver', ax=axes)\n",
        "\n",
        "# Plot capture sites with correct colors for each sector\n",
        "legend_handles = []  # Initialize list to store legend handles\n",
        "for sector, color in sector_color_dict.items():\n",
        "    if sector in capture_S0_regions['original_inventory_sector'].unique():\n",
        "        sector_data = capture_S0_regions[capture_S0_regions['original_inventory_sector'] == sector]\n",
        "        # Plot sector data with markers of fixed size for legend\n",
        "        handle = plt.scatter([], [], color=color, label=sector, s=100)  # Adjust the size (100) as needed\n",
        "        legend_handles.append(handle)  # Add handle to the list\n",
        "        # Plot sector data on the main plot with variable marker size\n",
        "        sector_data.plot(\n",
        "            ax=axes,\n",
        "            color=color,\n",
        "            markersize= sector_data['emissions_quantity'] / 1e4,\n",
        "        )\n",
        "\n",
        "# S0 cluster\n",
        "pipeline_handle = mlines.Line2D([], [], color='black', label='pipeline')\n",
        "legend_handles.append(pipeline_handle)  # Add pipeline handle to the list\n",
        "pipelines_S0_regions.plot(\n",
        "    ax=axes,\n",
        "    color='black',\n",
        "    linewidth=pipelines_S0_regions[\"qtrans\"] * 1.2\n",
        ")\n",
        "\n",
        "plt.savefig(NOTEBOOK_PATH/'results'/'figures'/'Figure_4_cluster.png', bbox_inches=\"tight\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSJssbxICjmR"
      },
      "source": [
        "# Bankruptcy results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PId_TDbOCjmR"
      },
      "source": [
        "## Import MILP results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyJkuoTDCjmR"
      },
      "outputs": [],
      "source": [
        "milp_res_df = pipelines_S0_regions[[\"start_pt\",\"capture_tc\",\"pipe_tc\"]]\n",
        "\n",
        "# Use variables of the paper\n",
        "milp_res_df.set_index(\"start_pt\",inplace=True)\n",
        "milp_res_df = milp_res_df.rename(columns={\n",
        "        \"capture_tc\":\"xi\",\n",
        "        \"pipe_tc\":\"yipi\"\n",
        "    })\n",
        "\n",
        "# Add Shapley value column (calculated by hand separately)\n",
        "sha_values = {\n",
        "    'C22': 0.13,\n",
        "    'C8': 1.22,\n",
        "    'C33': 1.23,\n",
        "    'C25': 1.83,\n",
        "    'C29': 6.39,\n",
        "    'C26': 2.01,\n",
        "    'C37': 4.85,\n",
        "    'C35': 8.41\n",
        "}\n",
        "milp_res_df[\"Shi\"] = milp_res_df.index.map(sha_values)\n",
        "\n",
        "# Add ci column\n",
        "milp_res_df[\"ci\"] = milp_res_df[\"xi\"] + milp_res_df[\"Shi\"]\n",
        "\n",
        "# Round values\n",
        "milp_res_df = milp_res_df.round(2)\n",
        "\n",
        "# Define budget (fixed value in paper)\n",
        "budget = 170\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz7BqjEECjmR"
      },
      "outputs": [],
      "source": [
        "# For readability with paper, we re-order the rows\n",
        "re_order = ['C22', 'C8', 'C33', 'C25', 'C29', 'C26', 'C37', 'C35']\n",
        "milp_res_df = milp_res_df.reindex(re_order)\n",
        "milp_res_df.round(2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE8DVTXNCjmS"
      },
      "source": [
        "## P solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9FQPNJdCjmS"
      },
      "outputs": [],
      "source": [
        "# Proportional solution\n",
        "\n",
        "# Calculate proportional allocation with the network\n",
        "milp_res_df[\"P_c\"] = (milp_res_df[\"ci\"] / milp_res_df[\"ci\"].sum() * budget).round(2)\n",
        "\n",
        "# Calculate proportional allocation without the network\n",
        "milp_res_df[\"P_x\"] = (milp_res_df[\"xi\"] / milp_res_df[\"xi\"].sum() * budget).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEvPFBvbCjmS"
      },
      "source": [
        "## CEA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoheh3EKCjmS"
      },
      "outputs": [],
      "source": [
        "# CEA solution (with network)\n",
        "\n",
        "# Create a working copy to track remaining claimants\n",
        "milp_res_df['CEA_c'] = 0.0  # Initialize CEA column\n",
        "working_df = milp_res_df.copy()\n",
        "working_df['remaining_claim'] = working_df['ci']\n",
        "\n",
        "budget_copy = budget\n",
        "\n",
        "while budget_copy > 0 and not working_df.empty:\n",
        "    n = len(working_df)\n",
        "    if n == 0:\n",
        "        break\n",
        "    share = budget_copy / n\n",
        "\n",
        "    # We find who can take the full share without exceeding their claim\n",
        "    fulfilled = working_df['remaining_claim'] <= share\n",
        "    not_fulfilled = ~fulfilled\n",
        "\n",
        "    # Allocate to fully fulfilled\n",
        "    milp_res_df.loc[working_df[fulfilled].index, 'CEA_c'] += working_df[fulfilled]['remaining_claim']\n",
        "    budget_copy -= working_df[fulfilled]['remaining_claim'].sum()\n",
        "\n",
        "    # Allocate share to others\n",
        "    milp_res_df.loc[working_df[not_fulfilled].index, 'CEA_c'] += share\n",
        "    budget_copy -= share * not_fulfilled.sum()\n",
        "\n",
        "    # Update remaining claims\n",
        "    working_df.loc[not_fulfilled, 'remaining_claim'] -= share\n",
        "    working_df = working_df[not_fulfilled]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF4vxSGyCjmS"
      },
      "outputs": [],
      "source": [
        "# CEA solution (without network)\n",
        "\n",
        "budget_copy = budget\n",
        "# Create a working copy to track remaining claimants\n",
        "milp_res_df['CEA_x'] = 0.0  # Initialize CEA_x column\n",
        "working_df = milp_res_df.copy()\n",
        "working_df['remaining_claim'] = working_df['xi']\n",
        "\n",
        "while budget_copy > 0 and not working_df.empty:\n",
        "    n = len(working_df)\n",
        "    if n == 0:\n",
        "        break\n",
        "    share = budget_copy / n\n",
        "\n",
        "    # Find who can take the full share without exceeding their claim\n",
        "    fulfilled = working_df['remaining_claim'] <= share #returns a boolean series\n",
        "    not_fulfilled = ~fulfilled\n",
        "\n",
        "    # Allocate to fully fulfilled\n",
        "    milp_res_df.loc[working_df[fulfilled].index, 'CEA_x'] += working_df[fulfilled]['remaining_claim']\n",
        "    budget_copy -= working_df[fulfilled]['remaining_claim'].sum()\n",
        "\n",
        "    # Allocate share to others\n",
        "    milp_res_df.loc[working_df[not_fulfilled].index, 'CEA_x'] += share\n",
        "    budget_copy -= share * not_fulfilled.sum()\n",
        "\n",
        "    # Update remaining claims\n",
        "    working_df.loc[not_fulfilled, 'remaining_claim'] -= share\n",
        "    working_df = working_df[not_fulfilled]  # Remove fulfilled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhRDENlcCjmS"
      },
      "source": [
        "## CEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYnqesJMCjmW"
      },
      "outputs": [],
      "source": [
        "# CEL solution (with network)\n",
        "milp_res_df['CEL_c'] = milp_res_df['ci']  # Start with full claims\n",
        "working_df = milp_res_df.copy()\n",
        "working_df['remaining_award'] = working_df['ci']\n",
        "total_claims = working_df['ci'].sum()\n",
        "losses_to_distribute = total_claims - budget\n",
        "\n",
        "while losses_to_distribute > 0 and not working_df.empty:\n",
        "    n = len(working_df)\n",
        "    if n == 0:\n",
        "        break\n",
        "    loss_share = losses_to_distribute / n\n",
        "\n",
        "    # Who will be reduced to zero (can't lose full share)?\n",
        "    exhausted = working_df['remaining_award'] <= loss_share\n",
        "    not_exhausted = ~exhausted\n",
        "\n",
        "    # Fully remove exhausted ones (set award to 0)\n",
        "    milp_res_df.loc[working_df[exhausted].index, 'CEL_c'] -= working_df[exhausted]['remaining_award']\n",
        "    losses_to_distribute -= working_df[exhausted]['remaining_award'].sum()\n",
        "\n",
        "    # Remove partial loss from others\n",
        "    milp_res_df.loc[working_df[not_exhausted].index, 'CEL_c'] -= loss_share\n",
        "    losses_to_distribute -= loss_share * not_exhausted.sum()\n",
        "\n",
        "    # Update working_df for next iteration\n",
        "    working_df.loc[not_exhausted, 'remaining_award'] -= loss_share\n",
        "    working_df = working_df[not_exhausted]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KVn7ZDhCjmX"
      },
      "outputs": [],
      "source": [
        "# CEL solution (without network)\n",
        "milp_res_df['CEL_x'] = milp_res_df['xi']  # Start with full claims\n",
        "working_df = milp_res_df.copy()\n",
        "working_df['remaining_award'] = working_df['xi']\n",
        "total_claims = working_df['xi'].sum()\n",
        "losses_to_distribute = total_claims - budget\n",
        "\n",
        "while losses_to_distribute > 0 and not working_df.empty:\n",
        "    n = len(working_df)\n",
        "    if n == 0:\n",
        "        break\n",
        "    loss_share = losses_to_distribute / n\n",
        "\n",
        "    # Who will be reduced to zero (can't lose full share)?\n",
        "    exhausted = working_df['remaining_award'] <= loss_share\n",
        "    not_exhausted = ~exhausted\n",
        "\n",
        "    # Fully remove exhausted ones (set award to 0)\n",
        "    milp_res_df.loc[working_df[exhausted].index, 'CEL_c'] -= working_df[exhausted]['remaining_award']\n",
        "    losses_to_distribute -= working_df[exhausted]['remaining_award'].sum()\n",
        "\n",
        "    # Remove partial loss from others\n",
        "    milp_res_df.loc[working_df[not_exhausted].index, 'CEL_x'] -= loss_share\n",
        "    losses_to_distribute -= loss_share * not_exhausted.sum()\n",
        "\n",
        "    # Update working_df for next iteration\n",
        "    working_df.loc[not_exhausted, 'remaining_award'] -= loss_share\n",
        "    working_df = working_df[not_exhausted]  # Drop those fully exhausted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qwvKc1ZCjmX"
      },
      "source": [
        "## Bankruptcy results in a single table\n",
        "\n",
        "All results are computed in res_df which is the basis for the various tables shown in the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LaRg4bWCjmX"
      },
      "outputs": [],
      "source": [
        "milp_res_df[\"CEA/ci\"] = milp_res_df[\"CEA_c\"] / milp_res_df[\"ci\"]\n",
        "milp_res_df[\"CEL/ci\"] = milp_res_df[\"CEL_c\"] / milp_res_df[\"ci\"]\n",
        "milp_res_df[\"P/ci\"] = milp_res_df[\"P_c\"] / milp_res_df[\"ci\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5fR0kzCCjmX"
      },
      "outputs": [],
      "source": [
        "milp_res_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p96-Wc4fCjmX"
      },
      "outputs": [],
      "source": [
        "res_df = milp_res_df[[\"CEA_c\",\"CEL_c\",\"P_c\",\"CEA_x\",\"CEL_x\",\"P_x\",\"xi\",\"yipi\",\"Shi\",\"ci\", \"CEA/ci\", \"CEL/ci\", \"P/ci\"]].T.round(2)\n",
        "res_cea = res_df.loc[[\"CEA_c\",\"CEA_x\"]]\n",
        "res_P   = res_df.loc[[\"P_c\",\"P_x\",\"xi\",\"ci\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-ZBxv9hCjmX"
      },
      "outputs": [],
      "source": [
        "res_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igNqbVadCjmY"
      },
      "source": [
        "# Tables\n",
        "\n",
        "Please find below all tables presented in the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X02Eab8cCjmY"
      },
      "source": [
        "## Table 3\n",
        "\n",
        "Note that in the paper we also add a row sith successors P(i). This is done by hand."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZxwrKImCjmY"
      },
      "outputs": [],
      "source": [
        "res_df.loc[[\"xi\",\"yipi\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l7_-k8ZCjmY"
      },
      "source": [
        "## Table D.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9uDFq7VCjmY"
      },
      "outputs": [],
      "source": [
        "res_df.loc[[\"CEA_c\",\"CEL_c\",\"P_c\",\"CEA_x\",\"CEL_x\",\"P_x\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0XCaS_VCjmY"
      },
      "source": [
        "## Table D.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kA9AHoKCjmY"
      },
      "outputs": [],
      "source": [
        "res_df.loc[[\"CEA/ci\", \"CEL/ci\", \"P/ci\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa4AXqodCjmZ"
      },
      "source": [
        "# Graph results\n",
        "\n",
        "Figures 6 and 7 are presented below. By uncommenting the code in the cell \"CEA figure\" one can also display the results for the CEA case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN4GME9sCjmZ"
      },
      "source": [
        "## Figure 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMhsd8NUCjmZ"
      },
      "outputs": [],
      "source": [
        "# Figure 6 in paper (revised verson V2)\n",
        "# Specify colors for P_x and P_c using RGB tuples\n",
        "p_colors = [(31, 119, 180), (255, 127, 14)]  # Example RGB tuples\n",
        "\n",
        "# Normalize RGB values to be between 0 and 1\n",
        "p_colors = [(r / 255, g / 255, b / 255) for r, g, b in p_colors]\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(res_P.columns))\n",
        "\n",
        "bar1 = ax.bar(index, res_P.loc['P_x'], bar_width, label='$P_{i}(N,x,\\mathcal{E})$', color=p_colors[0])\n",
        "bar2 = ax.bar(index + bar_width, res_P.loc['P_c'], bar_width, label='$P_{i}(N,c,\\mathcal{E})$', color=p_colors[1])\n",
        "\n",
        "ax.set_xlabel('Emitters')\n",
        "ax.set_ylabel('Subsidy [M€/y]')\n",
        "ax.set_xticks(index + bar_width / 2)\n",
        "ax.set_xticklabels(res_P.columns)\n",
        "\n",
        "# Adding y ticks at intervals of 5\n",
        "ax.set_yticks(np.arange(0, max(res_P.values.max(), 5) + 5, 5))\n",
        "\n",
        "ax.legend(title='Legend')\n",
        "\n",
        "# plt.savefig('P_plot_v2.pgf',bbox_inches = \"tight\")\n",
        "plt.savefig(NOTEBOOK_PATH/'results'/'figures'/'Figure_6.png', bbox_inches=\"tight\")\n",
        "\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiIS7g-NCjmZ"
      },
      "source": [
        "## Figure 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSMeIR3lCjmZ"
      },
      "outputs": [],
      "source": [
        "# Definition of regret (see paper)\n",
        "regret_x = (res_P.loc['ci'] - res_P.loc['P_x']) / res_P.loc['ci']\n",
        "regret_c = (res_P.loc['ci'] - res_P.loc['P_c']) / res_P.loc['ci']\n",
        "\n",
        "# Add the new row to the dataframe\n",
        "res_P.loc['regret_x'] = regret_x\n",
        "res_P.loc['regret_c'] = regret_c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dSGYPxCCjmZ"
      },
      "outputs": [],
      "source": [
        "res_P.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQRVQlVKCjmZ"
      },
      "outputs": [],
      "source": [
        "# Specify colors for P_x and P_c using RGB tuples\n",
        "p_colors = [(31, 119, 180), (255, 127, 14), (0, 0, 0)]  # Example RGB tuples\n",
        "\n",
        "# Normalize RGB values to be between 0 and 1\n",
        "p_colors = [(r / 255, g / 255, b / 255) for r, g, b in p_colors]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cWTESL-CjmZ"
      },
      "outputs": [],
      "source": [
        "#  Regrets plot with comparison to c_i\n",
        "# Plotting\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "bar_width = 0.35\n",
        "index = np.arange(len(res_P.columns))\n",
        "\n",
        "bar1 = ax.bar(index, res_P.loc['regret_x'], bar_width, label='$Regret_{i}(N,x,\\mathcal{E})$ (normalized)', color=p_colors[0])\n",
        "bar2 = ax.bar(index + bar_width, res_P.loc['regret_c'], bar_width, label='$Regret_{i}(N,c,\\mathcal{E})$ (normalized)', color=p_colors[1])\n",
        "\n",
        "ax.set_xlabel('Emitters')\n",
        "ax.set_ylabel('Regret [\\%]')\n",
        "ax.set_xticks(index + bar_width / 2)\n",
        "ax.set_xticklabels(res_P.columns)\n",
        "\n",
        "# Adding y ticks at intervals of 5\n",
        "# ax.set_yticks(np.arange(0, max(res_P.values.max(), 5) + 5, 5))\n",
        "ax.set_yticks(np.arange(0, 0.6, 0.1))\n",
        "\n",
        "ax.legend(title='Legend')\n",
        "\n",
        "# plt.savefig('P_plot_regret_v2.pgf',bbox_inches = \"tight\")\n",
        "plt.savefig(NOTEBOOK_PATH/'results'/'figures'/'Figure_7.png', bbox_inches=\"tight\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgVfuu5DCjmZ"
      },
      "source": [
        "## CEA figure\n",
        "\n",
        "Note that this figure is not in the paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz1wGcWfCjmZ"
      },
      "outputs": [],
      "source": [
        "# # Specify colors for CEA_x and CEA_c\n",
        "# cea_colors = [(31, 119, 180), (255, 127, 14)]\n",
        "\n",
        "# # Normalize RGB values to be between 0 and 1\n",
        "# cea_colors = [(r / 255, g / 255, b / 255) for r, g, b in cea_colors]\n",
        "\n",
        "# # Plotting\n",
        "# fig, ax = plt.subplots()\n",
        "\n",
        "# bar_width = 0.35\n",
        "# index = np.arange(len(res_cea.columns))\n",
        "\n",
        "# bar1 = ax.bar(index, res_cea.loc['CEA_x'], bar_width, label='$CEA_{i}(N,x,\\mathcal{E})$', color=cea_colors[0])\n",
        "# bar2 = ax.bar(index + bar_width, res_cea.loc['CEA_c'], bar_width, label='$CEA_{i}(N,c,\\mathcal{E})$', color=cea_colors[1])\n",
        "\n",
        "# ax.set_xlabel('Emitters')\n",
        "# ax.set_ylabel('Subsidy')\n",
        "# ax.set_xticks(index + bar_width / 2)\n",
        "# ax.set_xticklabels(res_cea.columns)\n",
        "\n",
        "# # Adding y ticks at intervals of 5\n",
        "# ax.set_yticks(np.arange(0, max(res_cea.values.max(), 5) + 5, 5))\n",
        "\n",
        "# ax.legend(title='Legend')\n",
        "\n",
        "# # plt.savefig('CEA_plot_v2.pgf',bbox_inches = \"tight\")\n",
        "\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEeB1RBOCjma"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnBrr5gqCjma"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHCzjChnCjma"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LT5-_PjRCjma"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}